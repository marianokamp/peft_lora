{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dd88ea-369a-4f4b-82cc-1529e7a85ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 31 17:39:51 CEST 2024\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78f4dfe-0196-4557-ab16-4386c244d727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mkamp/code/peft_lora/hf\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbb8ebc-f235-470d-8f53-5660b3eb47ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -Uq transformers datasets evaluate accelerate torch sagemaker boto3 botocore pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f376481d-e34a-477f-bad2-b5f04a3d4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48081996-4be8-474c-985e-c77412b0a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gen_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d241390-ea0f-485f-8a3b-f1385cc09bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import boto3\n",
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3303fc-84ea-4638-a21c-da6d14a84562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gen_src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile gen_src/requirements.txt\n",
    "torch~=2.2\n",
    "tiktoken\n",
    "bitsandbytes~=0.43\n",
    "peft~=0.10\n",
    "transformers~=4.39.2 \n",
    "accelerate~=0.28 \n",
    "datasets~=2.18\n",
    "evaluate~=0.4.1\n",
    "\n",
    "pynvml\n",
    "#tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd356777-a457-4c58-8792-9bb9a869e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gen_src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gen_src/train.py\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from threading import Thread\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    replace_lora_weights_loftq,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    IA3Config,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "logger.info(f'transformers {transformers.__version__}')\n",
    "logger.info(f'peft {peft.__version__}')\n",
    "logger.info(f'torch {torch.__version__}')\n",
    "\n",
    "def count_parameters(m, verbose=True):\n",
    "    total_count = 0\n",
    "    learnable_count = 0\n",
    "    if verbose:\n",
    "        logger.debug(\"Parameters (name, tunable, count):\")\n",
    "\n",
    "    output_width = max([len(n) for n, _ in m.named_parameters()])\n",
    "    for n, p in m.named_parameters():\n",
    "        count = p.data.numel()\n",
    "        if verbose:\n",
    "            logger.debug(f\" {n:{output_width}} {p.requires_grad:5b} {count:>11d}\")\n",
    "        total_count += count\n",
    "        if p.requires_grad:\n",
    "            learnable_count += count\n",
    "\n",
    "    logger.info(\n",
    "        f\"Total parameters: {total_count:,}, \"\n",
    "        f\"thereof learnable: {learnable_count:,} \"\n",
    "        f\"({learnable_count/total_count*100.:5.4f}%)\"\n",
    "    )\n",
    "\n",
    "    return total_count, learnable_count\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    " \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = load_accuracy.compute(\n",
    "        predictions=predictions, references=labels)[\"accuracy\"]\n",
    " \n",
    "    metrics = {f\"accuracy\": accuracy}\n",
    " \n",
    "    return metrics\n",
    "\n",
    "def fit(args):\n",
    "    ### model / tokenizer\n",
    "\n",
    "    # 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    conf = AutoConfig.from_pretrained(args.hf_ckp, num_labels=2)\n",
    "    print('conf', conf)\n",
    "    \n",
    "    ## model\n",
    "    \n",
    "    # Full bnb_config instead of just \"load_in_4bit\"?\n",
    "    \n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bits_use_double_quant=True\n",
    "    # )\n",
    "      \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.hf_ckp, \n",
    "        config=conf, \n",
    "        #torch_dtype=torch.bfloat16, \n",
    "        #load_in_8bit=True,\n",
    "        load_in_4bit=True,\n",
    "        #quantization_config=bnb_config\n",
    "    )\n",
    " \n",
    "    model = prepare_model_for_kbit_training(model) \n",
    "\n",
    "    if args.use_hf_lora:\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=4,\n",
    "            lora_alpha=4,\n",
    "            use_dora=False,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules='all-linear' # ['query_key_value','dense_h_to_4h','dense_4h_to_h']\n",
    "        ) \n",
    "                \n",
    "        # Use IA3Config instead of LoRA config?\n",
    "        \n",
    "        # peft_config = IA3Config(\n",
    "        #     task_type=TaskType.SEQ_CLS,\n",
    "        #     target_modules='all-linear' # \n",
    "        # )\n",
    "        \n",
    "        model = get_peft_model(model, peft_config)\n",
    "        #replace_lora_weights_loftq(model)\n",
    "    \n",
    "    count_parameters(model, verbose=args.use_hf_lora)\n",
    "    \n",
    "    #model = torch.compile(model)\n",
    "    \n",
    "    ## tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.hf_ckp)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    " \n",
    "    ### data \n",
    "    datasets.logging.disable_progress_bar()\n",
    "    dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "    train = dataset[\"train\"]\n",
    "    valid = dataset[\"validation\"]\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"sentence\"], padding=False, truncation=True)\n",
    "\n",
    "    tokenized_train = train.map(preprocess_function, batched=False, remove_columns=['idx', 'sentence'])\n",
    "    tokenized_valid = valid.map(preprocess_function, batched=False, remove_columns=['idx', 'sentence'])\n",
    "\n",
    "    ### Trainer\n",
    "    \n",
    "    log_steps = len(tokenized_train) // args.batch_size // 50 # 2 log outputs per epoch\n",
    "    use_bf16 = True if args.use_bf16 and torch.cuda.is_available() else False\n",
    "    print(f'Using bf16: {use_bf16}, LoRA: {args.use_hf_lora}')\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir if args.model_dir else \"out\",    \n",
    "        learning_rate=args.learning_rate,\n",
    "        #auto_find_batch_size=True,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.1,\n",
    "        push_to_hub=False,\n",
    "        bf16=use_bf16,\n",
    "        warmup_steps=150,\n",
    "        max_steps=600,\n",
    "        save_steps=log_steps,\n",
    "        logging_steps=log_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"no\", #\"steps\",\n",
    "        #save_total_limit=1,\n",
    "        \n",
    "        gradient_accumulation_steps=args.gradient_acc_steps,\n",
    "        gradient_checkpointing=args.use_gradient_checkpointing,\n",
    "        \n",
    "        disable_tqdm=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        #report_to=\"tensorboard\",\n",
    "        #logging_dir=\"/opt/ml/output/tensorboard\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # no early stopping for simplicity\n",
    "    )\n",
    "\n",
    " \n",
    "    ### train\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def schedule_gpu_memory_logging():\n",
    "        def log_gpu_usage():\n",
    "            if not torch.cuda.is_available():\n",
    "                return\n",
    "\n",
    "            from pynvml.smi import nvidia_smi\n",
    "\n",
    "            nvsmi = nvidia_smi.getInstance()\n",
    "            res = nvsmi.DeviceQuery(\"memory.free, memory.total, memory.used\")[\"gpu\"][0][\n",
    "                \"fb_memory_usage\"\n",
    "            ]\n",
    "            res[\"percentage\"] = res[\"used\"] / res[\"total\"] * 100\n",
    "            logger.info(\n",
    "                f'GPU Usage. Used: {res[\"used\"]:5.3f} Total: {res[\"total\"]:5.3f} ({res[\"percentage\"]:3.1f}% used). Free: {res[\"free\"]:5.3f}'\n",
    "            )\n",
    "        \n",
    "        def log_loop():\n",
    "            while True:\n",
    "                log_gpu_usage()\n",
    "                time.sleep(60)\n",
    "    \n",
    "        t = Thread(target=log_loop, daemon=True)\n",
    "        t.start()\n",
    "\n",
    "    schedule_gpu_memory_logging()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--use-bf16\", type=int, default=1, help='1 yes, 0 no') \n",
    "    parser.add_argument(\"--use-gradient-checkpointing\", type=int, default=1, help='1 yes, 0 no') \n",
    "    parser.add_argument(\"--use-hf-lora\", type=int, default=0, help='1 yes, 0 no')\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=4e-5)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=224)\n",
    "    parser.add_argument(\"--gradient-acc-steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--hf-ckp\", type=str, default='roberta-base')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    fit(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba11b5db-5372-45d5-bbf7-dff1488edf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Preferences/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/mkamp/Library/Preferences/sagemaker/config.yaml\n",
      "tb <sagemaker.debugger.debugger.TensorBoardOutputConfig object at 0x16dd862f0>\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import datetime\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "job_name = name_from_base('QLoRA', short=True)\n",
    "SAGEMAKER_BUCKET = sagemaker.Session().default_bucket()\n",
    "ymd = datetime.date.today().strftime('%Y/%m/%d')\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path              = f's3://{SAGEMAKER_BUCKET}/tensorboard/{ymd}/{job_name}',\n",
    "    container_local_output_path = \"/opt/ml/output/tensorboard\"\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    'use-bf16'  : 1,\n",
    "    'use-hf-lora': 1,\n",
    "    'learning-rate': 3e-5,\n",
    "    'batch-size': 128+32,\n",
    "    'gradient-acc-steps': 1, \n",
    "    'use-gradient-checkpointing': 0,\n",
    "    'hf-ckp': 'roberta-base'\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    \n",
    "    {'Name': 'train_samples_per_second', 'Regex': '\\'train_samples_per_second\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'valid_acc', 'Regex': '\\'eval_accuracy\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'train_loss', 'Regex': '\\'loss\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'valid_loss', 'Regex': '\\'eval_loss\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'gpu_mem', 'Regex': 'GPU Usage.*?(-?[0-9\\\\.]+)% used'}\n",
    "]\n",
    "print('tb', tensorboard_output_config)\n",
    "estimator_parameters = dict(\n",
    "    source_dir         = 'gen_src',\n",
    "    entry_point        = 'train.py',\n",
    "    instance_type      = 'ml.g5.xlarge',#'ml.g4dn.xlarge',\n",
    "    instance_count     = 1,\n",
    "    framework_version  = '2.2',\n",
    "    py_version         = 'py310',\n",
    "    use_spot_instances = True,\n",
    "    max_run            = 24*60*60, # one day in seconds\n",
    "    max_wait           = 24*60*60, \n",
    "    role               = get_execution_role(),\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters    = hyperparameters,\n",
    "    tensorboard_output_config = tensorboard_output_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6e29b0-7d7f-400d-878c-f210f09a727d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use-bf16': '1',\n",
       " 'use-hf-lora': '1',\n",
       " 'learning-rate': '3e-05',\n",
       " 'batch-size': '160',\n",
       " 'gradient-acc-steps': '1',\n",
       " 'use-gradient-checkpointing': '0',\n",
       " 'hf-ckp': '\"\\\\\"mistralai/Mistral-7B-Instruct-v0.2\\\\\"\"'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = PyTorch(**estimator_parameters)\n",
    "ckp = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "est.set_hyperparameters(**{'hf-ckp': ckp})\n",
    "est.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0181dd1e-2419-43aa-9b34-bce7148dfc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-753739741425/tensorboard/2024/03/31/QLoRA-240331-1739\n",
      "/opt/ml/output/tensorboard\n"
     ]
    }
   ],
   "source": [
    "print(est.tensorboard_output_config.s3_output_path)\n",
    "print(est.tensorboard_output_config.container_local_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102e246a-4743-4c62-a088-8122fe1573b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: QLoRA-240331-1739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-31 15:39:56 Starting - Starting the training job...\n",
      "2024-03-31 15:40:11 Starting - Preparing the instances for training......\n",
      "2024-03-31 15:41:06 Downloading - Downloading input data...\n",
      "2024-03-31 15:41:42 Downloading - Downloading the training image..................\n",
      "2024-03-31 15:44:48 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-31 15:44:55,509 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-31 15:44:55,527 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 15:44:55,537 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-31 15:44:55,544 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-31 15:44:56,835 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch~=2.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes~=0.43 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft~=0.10 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers~=4.39.2 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate~=0.28 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets~=2.18 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate~=0.4.1 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch~=2.2->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch~=2.2->-r requirements.txt (line 1)) (4.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch~=2.2->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch~=2.2->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch~=2.2->-r requirements.txt (line 1)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch~=2.2->-r requirements.txt (line 1)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2022.1.18 (from tiktoken->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 7.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes~=0.43->-r requirements.txt (line 3)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft~=0.10->-r requirements.txt (line 4)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft~=0.10->-r requirements.txt (line 4)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft~=0.10->-r requirements.txt (line 4)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft~=0.10->-r requirements.txt (line 4)) (4.66.1)\u001b[0m\n",
      "\u001b[34mCollecting safetensors (from peft~=0.10->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.17.0 (from peft~=0.10->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers~=4.39.2->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets~=2.18->-r requirements.txt (line 7)) (15.0.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets~=2.18->-r requirements.txt (line 7)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets~=2.18->-r requirements.txt (line 7)) (2.2.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets~=2.18->-r requirements.txt (line 7)) (0.70.16)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate~=0.4.1->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets~=2.18->-r requirements.txt (line 7)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets~=2.18->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch~=2.2->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets~=2.18->-r requirements.txt (line 7)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets~=2.18->-r requirements.txt (line 7)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets~=2.18->-r requirements.txt (line 7)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch~=2.2->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets~=2.18->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 88.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 MB 26.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 111.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 37.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 46.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 7.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 74.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 45.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 58.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 66.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 85.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 27.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 34.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, safetensors, regex, pynvml, pyarrow-hotfix, multidict, frozenlist, async-timeout, yarl, tiktoken, responses, huggingface-hub, aiosignal, tokenizers, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.28.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.43.0 datasets-2.18.0 evaluate-0.4.1 frozenlist-1.4.1 huggingface-hub-0.22.2 multidict-6.0.5 peft-0.10.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2023.12.25 responses-0.18.0 safetensors-0.4.2 tiktoken-0.6.0 tokenizers-0.15.2 transformers-4.39.2 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,474 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,474 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,511 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,541 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,570 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,582 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 160,\n",
      "        \"gradient-acc-steps\": 1,\n",
      "        \"hf-ckp\": \"\\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\",\n",
      "        \"learning-rate\": 3e-05,\n",
      "        \"use-bf16\": 1,\n",
      "        \"use-gradient-checkpointing\": 0,\n",
      "        \"use-hf-lora\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"QLoRA-240331-1739\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/QLoRA-240331-1739/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":160,\"gradient-acc-steps\":1,\"hf-ckp\":\"\\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\",\"learning-rate\":3e-05,\"use-bf16\":1,\"use-gradient-checkpointing\":0,\"use-hf-lora\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/QLoRA-240331-1739/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":160,\"gradient-acc-steps\":1,\"hf-ckp\":\"\\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\",\"learning-rate\":3e-05,\"use-bf16\":1,\"use-gradient-checkpointing\":0,\"use-hf-lora\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"QLoRA-240331-1739\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/QLoRA-240331-1739/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"160\",\"--gradient-acc-steps\",\"1\",\"--hf-ckp\",\"\\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\",\"--learning-rate\",\"3e-05\",\"--use-bf16\",\"1\",\"--use-gradient-checkpointing\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=160\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT-ACC-STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=\"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-GRADIENT-CHECKPOINTING=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 160 --gradient-acc-steps 1 --hf-ckp \"mistralai/Mistral-7B-Instruct-v0.2\" --learning-rate 3e-05 --use-bf16 1 --use-gradient-checkpointing 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,583 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-03-31 15:45:12,583 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.39.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.10.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.2.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 519.688 Total: 23028.000 (2.3% used). Free: 22508.312\u001b[0m\n",
      "\u001b[34mloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\u001b[0m\n",
      "\u001b[34mloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\u001b[0m\n",
      "\u001b[34mModel config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mModel config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mconf\u001b[0m\n",
      "\u001b[34mMistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\u001b[0m\n",
      "\u001b[34mThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\u001b[0m\n",
      "\u001b[34mThe device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34mThe device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34mloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 1/3 [00:24<00:48, 24.30s/it]\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 519.688 Total: 23028.000 (2.3% used). Free: 22508.312\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 2/3 [01:01<00:31, 31.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 3/3 [01:34<00:00, 32.51s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 3/3 [01:34<00:00, 31.60s/it]\u001b[0m\n",
      "\u001b[34mInstantiating MistralForSequenceClassification model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34mInstantiating MistralForSequenceClassification model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1517.000 Total: 23028.000 (6.6% used). Free: 21511.000\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:48<01:36, 48.48s/it]\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3715.000 Total: 23028.000 (16.1% used). Free: 19313.000\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [01:25<00:41, 41.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [01:56<00:00, 37.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [01:56<00:00, 38.97s/it]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 were not used when initializing MistralForSequenceClassification: ['lm_head.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing MistralForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing MistralForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 were not used when initializing MistralForSequenceClassification: ['lm_head.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing MistralForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing MistralForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.embed_tokens.weight                                  0   131072000\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.0.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.1.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.2.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.3.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.4.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.5.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.6.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.7.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.8.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight          0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight      1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight          0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight      1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight               0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight           1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight           1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight             0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight         1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight         1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.input_layernorm.weight                      0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.9.post_attention_layernorm.weight             0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.10.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.11.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.12.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.13.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.14.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.15.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.16.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.17.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.18.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.19.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.20.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.21.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.22.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.23.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.24.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.25.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.26.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.27.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.28.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.29.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.30.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight         0     2097152\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight     1        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight         0     8388608\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight     1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight              0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight          1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight          1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight            0    29360128\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight        1       57344\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.input_layernorm.weight                     0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.layers.31.post_attention_layernorm.weight            0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.model.norm.weight                                          0        4096\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.score.original_module.base_layer.weight                    0        8192\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.score.original_module.lora_A.default.weight                0       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.score.original_module.lora_B.default.weight                0           8\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.score.modules_to_save.default.base_layer.weight            1        8192\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.score.modules_to_save.default.lora_A.default.weight        1       16384\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.score.modules_to_save.default.lora_B.default.weight        1           8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 3,631,534,096, thereof learnable: 10,510,344 (0.2894%)\u001b[0m\n",
      "\u001b[34mloading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\u001b[0m\n",
      "\u001b[34mloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\u001b[0m\n",
      "\u001b[34mloading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34mloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mloading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\u001b[0m\n",
      "\u001b[34mloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\u001b[0m\n",
      "\u001b[34mloading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34mloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 15.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 15.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 396kB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 396kB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 148k/148k [00:00<00:00, 882kB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 148k/148k [00:00<00:00, 880kB/s]\u001b[0m\n",
      "\u001b[34mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\u001b[0m\n",
      "\u001b[34mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\u001b[0m\n",
      "\u001b[34mUsing bf16: True, LoRA: 1\u001b[0m\n",
      "\u001b[34musing `logging_steps` to initialize `eval_steps` to 8\u001b[0m\n",
      "\u001b[34musing `logging_steps` to initialize `eval_steps` to 8\u001b[0m\n",
      "\u001b[34mPyTorch: setting up devices\u001b[0m\n",
      "\u001b[34mPyTorch: setting up devices\u001b[0m\n",
      "\u001b[34mThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[34mThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mUsing auto half precision backend\u001b[0m\n",
      "\u001b[34mUsing auto half precision backend\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 67,349\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 67,349\n",
      "  Num Epochs = 2\u001b[0m\n",
      "\u001b[34mNum Epochs = 2\n",
      "  Instantaneous batch size per device = 160\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 160\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 160\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 160\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 10,510,344\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 10,510,344\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13553.000 Total: 23028.000 (58.9% used). Free: 9475.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34m{'loss': 4.0974, 'grad_norm': 145.23135375976562, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 27.8MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.107184410095215, 'eval_accuracy': 0.41857798165137616, 'eval_runtime': 21.5131, 'eval_samples_per_second': 40.533, 'eval_steps_per_second': 0.279, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34m{'loss': 3.8452, 'grad_norm': 113.10195922851562, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.163388252258301, 'eval_accuracy': 0.42775229357798167, 'eval_runtime': 21.3652, 'eval_samples_per_second': 40.814, 'eval_steps_per_second': 0.281, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34m{'loss': 3.8189, 'grad_norm': 95.63640594482422, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.332892417907715, 'eval_accuracy': 0.4380733944954128, 'eval_runtime': 21.3751, 'eval_samples_per_second': 40.795, 'eval_steps_per_second': 0.281, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34m{'loss': 3.1816, 'grad_norm': 48.97526168823242, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.6528213024139404, 'eval_accuracy': 0.44495412844036697, 'eval_runtime': 21.3464, 'eval_samples_per_second': 40.85, 'eval_steps_per_second': 0.281, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21209.000 Total: 23028.000 (92.1% used). Free: 1819.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19979.000 Total: 23028.000 (86.8% used). Free: 3049.000\u001b[0m\n",
      "\u001b[34m{'loss': 2.7457, 'grad_norm': 37.28563690185547, 'learning_rate': 8e-06, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.376835823059082, 'eval_accuracy': 0.4598623853211009, 'eval_runtime': 21.3593, 'eval_samples_per_second': 40.825, 'eval_steps_per_second': 0.281, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19979.000 Total: 23028.000 (86.8% used). Free: 3049.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19979.000 Total: 23028.000 (86.8% used). Free: 3049.000\u001b[0m\n",
      "\u001b[34m{'loss': 2.485, 'grad_norm': 30.247074127197266, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.4617760181427, 'eval_accuracy': 0.47477064220183485, 'eval_runtime': 21.3507, 'eval_samples_per_second': 40.842, 'eval_steps_per_second': 0.281, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19979.000 Total: 23028.000 (86.8% used). Free: 3049.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19979.000 Total: 23028.000 (86.8% used). Free: 3049.000\u001b[0m\n",
      "\u001b[34m{'loss': 1.9906, 'grad_norm': 42.061824798583984, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.8485406637191772, 'eval_accuracy': 0.5435779816513762, 'eval_runtime': 21.3845, 'eval_samples_per_second': 40.777, 'eval_steps_per_second': 0.281, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19979.000 Total: 23028.000 (86.8% used). Free: 3049.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19211.000 Total: 23028.000 (83.4% used). Free: 3817.000\u001b[0m\n",
      "\u001b[34m{'loss': 1.6573, 'grad_norm': 31.240968704223633, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.372849464416504, 'eval_accuracy': 0.6307339449541285, 'eval_runtime': 21.3533, 'eval_samples_per_second': 40.837, 'eval_steps_per_second': 0.281, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19213.000 Total: 23028.000 (83.4% used). Free: 3815.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19213.000 Total: 23028.000 (83.4% used). Free: 3815.000\u001b[0m\n",
      "\u001b[34m{'loss': 1.2303, 'grad_norm': 28.394472122192383, 'learning_rate': 1.44e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.000611662864685, 'eval_accuracy': 0.7201834862385321, 'eval_runtime': 21.3717, 'eval_samples_per_second': 40.802, 'eval_steps_per_second': 0.281, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19213.000 Total: 23028.000 (83.4% used). Free: 3815.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.9191, 'grad_norm': 24.960044860839844, 'learning_rate': 1.6e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19213.000 Total: 23028.000 (83.4% used). Free: 3815.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.7189317345619202, 'eval_accuracy': 0.783256880733945, 'eval_runtime': 21.3592, 'eval_samples_per_second': 40.826, 'eval_steps_per_second': 0.281, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 19213.000 Total: 23028.000 (83.4% used). Free: 3815.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21027.000 Total: 23028.000 (91.3% used). Free: 2001.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.7112, 'grad_norm': 24.582931518554688, 'learning_rate': 1.76e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.500637412071228, 'eval_accuracy': 0.8520642201834863, 'eval_runtime': 21.356, 'eval_samples_per_second': 40.832, 'eval_steps_per_second': 0.281, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21027.000 Total: 23028.000 (91.3% used). Free: 2001.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.6793, 'grad_norm': 23.171642303466797, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.36559197306632996, 'eval_accuracy': 0.9013761467889908, 'eval_runtime': 21.6891, 'eval_samples_per_second': 40.205, 'eval_steps_per_second': 0.277, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.5539, 'grad_norm': 10.07550048828125, 'learning_rate': 2.08e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3038772940635681, 'eval_accuracy': 0.9151376146788991, 'eval_runtime': 21.3823, 'eval_samples_per_second': 40.781, 'eval_steps_per_second': 0.281, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.4158, 'grad_norm': 21.06488609313965, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.29642021656036377, 'eval_accuracy': 0.9277522935779816, 'eval_runtime': 21.3718, 'eval_samples_per_second': 40.801, 'eval_steps_per_second': 0.281, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.298, 'grad_norm': 32.29787826538086, 'learning_rate': 2.4e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2440093457698822, 'eval_accuracy': 0.9334862385321101, 'eval_runtime': 21.3469, 'eval_samples_per_second': 40.849, 'eval_steps_per_second': 0.281, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.3358, 'grad_norm': 6.136098384857178, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22405801713466644, 'eval_accuracy': 0.9426605504587156, 'eval_runtime': 21.3743, 'eval_samples_per_second': 40.797, 'eval_steps_per_second': 0.281, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.3254, 'grad_norm': 24.659318923950195, 'learning_rate': 2.72e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.26878422498703003, 'eval_accuracy': 0.930045871559633, 'eval_runtime': 21.4034, 'eval_samples_per_second': 40.741, 'eval_steps_per_second': 0.28, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.2298, 'grad_norm': 5.454883575439453, 'learning_rate': 2.88e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1903868466615677, 'eval_accuracy': 0.9392201834862385, 'eval_runtime': 21.579, 'eval_samples_per_second': 40.41, 'eval_steps_per_second': 0.278, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.252, 'grad_norm': 15.815380096435547, 'learning_rate': 2.9998537860139564e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16010788083076477, 'eval_accuracy': 0.9518348623853211, 'eval_runtime': 21.3696, 'eval_samples_per_second': 40.806, 'eval_steps_per_second': 0.281, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1859, 'grad_norm': 23.49637794494629, 'learning_rate': 2.9963460753897364e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16675837337970734, 'eval_accuracy': 0.9529816513761468, 'eval_runtime': 21.3562, 'eval_samples_per_second': 40.831, 'eval_steps_per_second': 0.281, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1781, 'grad_norm': 5.906951904296875, 'learning_rate': 2.988172051971717e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15761315822601318, 'eval_accuracy': 0.9541284403669725, 'eval_runtime': 21.5076, 'eval_samples_per_second': 40.544, 'eval_steps_per_second': 0.279, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1834, 'grad_norm': 8.170710563659668, 'learning_rate': 2.975357206220079e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15366341173648834, 'eval_accuracy': 0.9541284403669725, 'eval_runtime': 21.3754, 'eval_samples_per_second': 40.795, 'eval_steps_per_second': 0.281, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.2009, 'grad_norm': 7.749641418457031, 'learning_rate': 2.9579415008678196e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15214118361473083, 'eval_accuracy': 0.9472477064220184, 'eval_runtime': 21.355, 'eval_samples_per_second': 40.834, 'eval_steps_per_second': 0.281, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1743, 'grad_norm': 11.020686149597168, 'learning_rate': 2.9359792462981007e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 872\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16335269808769226, 'eval_accuracy': 0.9472477064220184, 'eval_runtime': 21.3571, 'eval_samples_per_second': 40.83, 'eval_steps_per_second': 0.281, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1508, 'grad_norm': 20.244159698486328, 'learning_rate': 2.9095389311788626e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17144988477230072, 'eval_accuracy': 0.948394495412844, 'eval_runtime': 21.3611, 'eval_samples_per_second': 40.822, 'eval_steps_per_second': 0.281, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1496, 'grad_norm': 9.850744247436523, 'learning_rate': 2.8787030088828517e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1558053195476532, 'eval_accuracy': 0.9529816513761468, 'eval_runtime': 21.6478, 'eval_samples_per_second': 40.281, 'eval_steps_per_second': 0.277, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1652, 'grad_norm': 26.00829315185547, 'learning_rate': 2.8435676403591193e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1444888710975647, 'eval_accuracy': 0.9564220183486238, 'eval_runtime': 21.5261, 'eval_samples_per_second': 40.509, 'eval_steps_per_second': 0.279, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1426, 'grad_norm': 18.798030853271484, 'learning_rate': 2.8042423942578285e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 872\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14979462325572968, 'eval_accuracy': 0.9529816513761468, 'eval_runtime': 21.3756, 'eval_samples_per_second': 40.794, 'eval_steps_per_second': 0.281, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1593, 'grad_norm': 25.733272552490234, 'learning_rate': 2.7608499052435265e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20966367423534393, 'eval_accuracy': 0.9472477064220184, 'eval_runtime': 21.5887, 'eval_samples_per_second': 40.391, 'eval_steps_per_second': 0.278, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1554, 'grad_norm': 21.749357223510742, 'learning_rate': 2.7135254915624213e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1418694257736206, 'eval_accuracy': 0.9541284403669725, 'eval_runtime': 21.4256, 'eval_samples_per_second': 40.699, 'eval_steps_per_second': 0.28, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1736, 'grad_norm': 15.463643074035645, 'learning_rate': 2.6624167330562697e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1505301594734192, 'eval_accuracy': 0.9564220183486238, 'eval_runtime': 21.3554, 'eval_samples_per_second': 40.833, 'eval_steps_per_second': 0.281, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1251, 'grad_norm': 9.514727592468262, 'learning_rate': 2.607683010938826e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1520220935344696, 'eval_accuracy': 0.9598623853211009, 'eval_runtime': 21.3695, 'eval_samples_per_second': 40.806, 'eval_steps_per_second': 0.281, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1497, 'grad_norm': 29.163227081298828, 'learning_rate': 2.5494950107700482e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17862722277641296, 'eval_accuracy': 0.9495412844036697, 'eval_runtime': 21.3552, 'eval_samples_per_second': 40.833, 'eval_steps_per_second': 0.281, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1561, 'grad_norm': 11.408482551574707, 'learning_rate': 2.4880341901780205e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1394650936126709, 'eval_accuracy': 0.9518348623853211, 'eval_runtime': 21.4186, 'eval_samples_per_second': 40.712, 'eval_steps_per_second': 0.28, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.14, 'grad_norm': 6.882279396057129, 'learning_rate': 2.4234922129884873e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13887274265289307, 'eval_accuracy': 0.9541284403669725, 'eval_runtime': 21.367, 'eval_samples_per_second': 40.811, 'eval_steps_per_second': 0.281, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1431, 'grad_norm': 7.370416164398193, 'learning_rate': 2.356070351526648e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16813625395298004, 'eval_accuracy': 0.9415137614678899, 'eval_runtime': 21.3578, 'eval_samples_per_second': 40.828, 'eval_steps_per_second': 0.281, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1432, 'grad_norm': 14.358600616455078, 'learning_rate': 2.285978858955119e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1348075121641159, 'eval_accuracy': 0.9564220183486238, 'eval_runtime': 21.3531, 'eval_samples_per_second': 40.837, 'eval_steps_per_second': 0.281, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22869.000 Total: 23028.000 (99.3% used). Free: 159.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1264, 'grad_norm': 10.813004493713379, 'learning_rate': 2.213436313605413e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14963814616203308, 'eval_accuracy': 0.9518348623853211, 'eval_runtime': 21.3644, 'eval_samples_per_second': 40.816, 'eval_steps_per_second': 0.281, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1245, 'grad_norm': 10.95272445678711, 'learning_rate': 2.138668937347609e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.125453919172287, 'eval_accuracy': 0.9610091743119266, 'eval_runtime': 21.3497, 'eval_samples_per_second': 40.844, 'eval_steps_per_second': 0.281, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1349, 'grad_norm': 6.380873680114746, 'learning_rate': 2.0619098901238684e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1276073157787323, 'eval_accuracy': 0.9598623853211009, 'eval_runtime': 21.3539, 'eval_samples_per_second': 40.836, 'eval_steps_per_second': 0.281, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1377, 'grad_norm': 7.793495178222656, 'learning_rate': 1.983398542845767e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11834124475717545, 'eval_accuracy': 0.9610091743119266, 'eval_runtime': 21.4853, 'eval_samples_per_second': 40.586, 'eval_steps_per_second': 0.279, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.122, 'grad_norm': 2.8979103565216064, 'learning_rate': 1.9033797309228984e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11972742527723312, 'eval_accuracy': 0.9610091743119266, 'eval_runtime': 21.3679, 'eval_samples_per_second': 40.809, 'eval_steps_per_second': 0.281, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1235, 'grad_norm': 9.339510917663574, 'learning_rate': 1.822102990750595e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12193647772073746, 'eval_accuracy': 0.9610091743119266, 'eval_runtime': 21.36, 'eval_samples_per_second': 40.824, 'eval_steps_per_second': 0.281, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.125, 'grad_norm': 9.152212142944336, 'learning_rate': 1.7398217815377526e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.121534064412117, 'eval_accuracy': 0.9610091743119266, 'eval_runtime': 21.3584, 'eval_samples_per_second': 40.827, 'eval_steps_per_second': 0.281, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1199, 'grad_norm': 15.638106346130371, 'learning_rate': 1.6567926949014805e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12412209808826447, 'eval_accuracy': 0.9621559633027523, 'eval_runtime': 21.3841, 'eval_samples_per_second': 40.778, 'eval_steps_per_second': 0.281, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1268, 'grad_norm': 27.478132247924805, 'learning_rate': 1.57327465469342e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13709548115730286, 'eval_accuracy': 0.9541284403669725, 'eval_runtime': 21.3487, 'eval_samples_per_second': 40.846, 'eval_steps_per_second': 0.281, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1338, 'grad_norm': 24.28693962097168, 'learning_rate': 1.4895281095530577e-05, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12163306027650833, 'eval_accuracy': 0.966743119266055, 'eval_runtime': 21.4026, 'eval_samples_per_second': 40.743, 'eval_steps_per_second': 0.28, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1251, 'grad_norm': 1.691085934638977, 'learning_rate': 1.40581422070603e-05, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 872\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12879061698913574, 'eval_accuracy': 0.9564220183486238, 'eval_runtime': 21.3743, 'eval_samples_per_second': 40.797, 'eval_steps_per_second': 0.281, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1185, 'grad_norm': 24.647920608520508, 'learning_rate': 1.3223940475402485e-05, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13689987361431122, 'eval_accuracy': 0.9621559633027523, 'eval_runtime': 21.6164, 'eval_samples_per_second': 40.34, 'eval_steps_per_second': 0.278, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1254, 'grad_norm': 13.741471290588379, 'learning_rate': 1.2395277334996045e-05, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14494748413562775, 'eval_accuracy': 0.9529816513761468, 'eval_runtime': 21.4693, 'eval_samples_per_second': 40.616, 'eval_steps_per_second': 0.279, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1466, 'grad_norm': 16.314672470092773, 'learning_rate': 1.1574736948340163e-05, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1257503181695938, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.3613, 'eval_samples_per_second': 40.821, 'eval_steps_per_second': 0.281, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1115, 'grad_norm': 9.556857109069824, 'learning_rate': 1.0764878147356852e-05, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1239369660615921, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.3784, 'eval_samples_per_second': 40.789, 'eval_steps_per_second': 0.281, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1272, 'grad_norm': 13.307144165039062, 'learning_rate': 9.968226453746177e-06, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12570911645889282, 'eval_accuracy': 0.9575688073394495, 'eval_runtime': 21.3495, 'eval_samples_per_second': 40.844, 'eval_steps_per_second': 0.281, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1095, 'grad_norm': 3.541182518005371, 'learning_rate': 9.187266203218457e-06, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12101913243532181, 'eval_accuracy': 0.9587155963302753, 'eval_runtime': 21.3599, 'eval_samples_per_second': 40.824, 'eval_steps_per_second': 0.281, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1185, 'grad_norm': 17.69782829284668, 'learning_rate': 8.424432798163838e-06, 'epoch': 1.05}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11761634796857834, 'eval_accuracy': 0.9621559633027523, 'eval_runtime': 21.3569, 'eval_samples_per_second': 40.83, 'eval_steps_per_second': 0.281, 'epoch': 1.05}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1042, 'grad_norm': 3.0541465282440186, 'learning_rate': 7.682105112919007e-06, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12122522294521332, 'eval_accuracy': 0.9621559633027523, 'eval_runtime': 21.3769, 'eval_samples_per_second': 40.792, 'eval_steps_per_second': 0.281, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1023, 'grad_norm': 24.478160858154297, 'learning_rate': 6.962598075315047e-06, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11929436773061752, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.3639, 'eval_samples_per_second': 40.817, 'eval_steps_per_second': 0.281, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1098, 'grad_norm': 2.0406057834625244, 'learning_rate': 6.26815544764066e-06, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12110038101673126, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.3676, 'eval_samples_per_second': 40.809, 'eval_steps_per_second': 0.281, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.087, 'grad_norm': 7.103715896606445, 'learning_rate': 5.600942829533097e-06, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1187334880232811, 'eval_accuracy': 0.966743119266055, 'eval_runtime': 21.3543, 'eval_samples_per_second': 40.835, 'eval_steps_per_second': 0.281, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1024, 'grad_norm': 10.136008262634277, 'learning_rate': 4.963040904617131e-06, 'epoch': 1.14}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11664436012506485, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.37, 'eval_samples_per_second': 40.805, 'eval_steps_per_second': 0.281, 'epoch': 1.14}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1147, 'grad_norm': 11.567485809326172, 'learning_rate': 4.356438951952189e-06, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11761488765478134, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.3792, 'eval_samples_per_second': 40.787, 'eval_steps_per_second': 0.281, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1247, 'grad_norm': 3.1709792613983154, 'learning_rate': 3.783028642522024e-06, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11389844119548798, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.356, 'eval_samples_per_second': 40.832, 'eval_steps_per_second': 0.281, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1216, 'grad_norm': 8.541549682617188, 'learning_rate': 3.244598140112404e-06, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11849965155124664, 'eval_accuracy': 0.9621559633027523, 'eval_runtime': 21.3661, 'eval_samples_per_second': 40.812, 'eval_steps_per_second': 0.281, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.109, 'grad_norm': 9.216468811035156, 'learning_rate': 2.7428265249730726e-06, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11803130060434341, 'eval_accuracy': 0.9610091743119266, 'eval_runtime': 21.3835, 'eval_samples_per_second': 40.779, 'eval_steps_per_second': 0.281, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1118, 'grad_norm': 2.245730400085449, 'learning_rate': 2.279278557653611e-06, 'epoch': 1.24}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12237513065338135, 'eval_accuracy': 0.9552752293577982, 'eval_runtime': 21.3687, 'eval_samples_per_second': 40.807, 'eval_steps_per_second': 0.281, 'epoch': 1.24}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1282, 'grad_norm': 6.391828536987305, 'learning_rate': 1.8553997993420495e-06, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11546209454536438, 'eval_accuracy': 0.963302752293578, 'eval_runtime': 21.3655, 'eval_samples_per_second': 40.813, 'eval_steps_per_second': 0.281, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21961.000 Total: 23028.000 (95.4% used). Free: 1067.000\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1084, 'grad_norm': 11.829254150390625, 'learning_rate': 1.4725121039232948e-06, 'epoch': 1.27}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11462266743183136, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.3705, 'eval_samples_per_second': 40.804, 'eval_steps_per_second': 0.281, 'epoch': 1.27}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.0986, 'grad_norm': 6.011826992034912, 'learning_rate': 1.1318094958153047e-06, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11575040966272354, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.3566, 'eval_samples_per_second': 40.83, 'eval_steps_per_second': 0.281, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1136, 'grad_norm': 3.243788242340088, 'learning_rate': 8.34354446437785e-07, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11560774594545364, 'eval_accuracy': 0.9644495412844036, 'eval_runtime': 21.377, 'eval_samples_per_second': 40.791, 'eval_steps_per_second': 0.281, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1052, 'grad_norm': 6.145270824432373, 'learning_rate': 5.810745609252166e-07, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11441988497972488, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.3428, 'eval_samples_per_second': 40.857, 'eval_steps_per_second': 0.281, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.0911, 'grad_norm': 1.960334062576294, 'learning_rate': 3.7275968541655104e-07, 'epoch': 1.35}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.114522285759449, 'eval_accuracy': 0.966743119266055, 'eval_runtime': 21.4924, 'eval_samples_per_second': 40.572, 'eval_steps_per_second': 0.279, 'epoch': 1.35}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1182, 'grad_norm': 15.092281341552734, 'learning_rate': 2.1005944394242694e-07, 'epoch': 1.37}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11441530287265778, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.368, 'eval_samples_per_second': 40.809, 'eval_steps_per_second': 0.281, 'epoch': 1.37}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.0851, 'grad_norm': 5.054840564727783, 'learning_rate': 9.348121259105447e-08, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11477326601743698, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.3851, 'eval_samples_per_second': 40.776, 'eval_steps_per_second': 0.281, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.0893, 'grad_norm': 1.6298352479934692, 'learning_rate': 2.338853727028467e-08, 'epoch': 1.41}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 872\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mBatch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1143048107624054, 'eval_accuracy': 0.9655963302752294, 'eval_runtime': 21.3555, 'eval_samples_per_second': 40.833, 'eval_steps_per_second': 0.281, 'epoch': 1.41}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\u001b[34m{'loss': 0.1144, 'grad_norm': 13.748327255249023, 'learning_rate': 0.0, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11434530466794968, 'eval_accuracy': 0.966743119266055, 'eval_runtime': 21.3527, 'eval_samples_per_second': 40.838, 'eval_steps_per_second': 0.281, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 8763.8945, 'train_samples_per_second': 10.954, 'train_steps_per_second': 0.068, 'train_loss': 0.4939296314120293, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mNum examples = 872\n",
      "  Batch size = 160\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22591.000 Total: 23028.000 (98.1% used). Free: 437.000\u001b[0m\n",
      "\n",
      "2024-03-31 18:15:51 Uploading - Uploading generated training model\u001b[34m{'eval_loss': 0.11434530466794968, 'eval_accuracy': 0.966743119266055, 'eval_runtime': 21.3536, 'eval_samples_per_second': 40.836, 'eval_steps_per_second': 0.281, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m2024-03-31 18:15:44,902 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:15:44,902 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:15:44,902 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-03-31 18:16:07 Completed - Training job completed\n",
      "Training seconds: 9301\n",
      "Billable seconds: 4149\n",
      "Managed Spot Training savings: 55.4%\n"
     ]
    }
   ],
   "source": [
    "est.fit(job_name=job_name, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
